I、 圖像顯著性任務

1. 設計思考 

我們將物件偵測模型 YOLOv3[\[1\]](#_page15_x68.00_y106.92)使用於顯著性的預測上，我們認為物件偵測與 顯著性有一定的關聯性存在，YOLOv3 中的多解析度特徵圖也適合用於不同解析度 的顯著性預測。 

2. 模型架構  ![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.001.png)

如圖 1 所示，藍色部分為原始 YOLOv3 架構，而綠色部分是針對顯著圖預測所做 出的修正，預測內容為顯著圖的區域均值 與標準差。 

圖  1  模型架構 

3. 訓練資料處理

我們使用 SALICON[\[2\]](#_page15_x68.00_y138.92)圖像顯著性資料集作為訓練資料，SALICON 圖像來自 MS COC[O\[3\]](#_page15_x68.00_y173.92)資料集，由 Jiang et al.蒐集創建，用於靜態圖像視覺顯著性任務，共 10000 筆訓練資料、5000 筆驗證資料、5000 筆測試資料，影像大小為 480x640，標 籤包含注視點及顯著圖。SALICON 資料量較多、品質較好且圖像內容多元，因此 我們選用此資料集。 

首先將顯著圖(Saliency map)做低階析度轉換，比起像素的正確性，我們更著重 於區塊顯著程度，我們使用三種不同大小的方格區域，在 8x8、16x16、32x32 區域 內算出平均及標準差，將顯著圖先縮放成 224x320 大小，然後轉為 28x40、14x20、 7x10 三種解析度，如圖 2 所示，我們讓模型學習預測三種解析度的平均與標準差， 平均高表示區塊內的顯著性越高，標準差小表示區域內顯著性集中，以此來近似高 析度的顯著性圖像。

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.002.jpeg)

圖  2 (a)輸入影像以及對應的顯著圖、注視點圖(注視點相當微小)，(b)、(c)顯著圖的區域均值、

標準差，區域的範圍如 YOLOv3 的輸出 cell 範圍一致，由上而下為 8x8, 16x16, 32x32，(d) 顯著圖區域均值與原圖疊合。

4. 訓練方法 

模型預測為三種不同尺度的均值、標準差，損失函數為 Binary Cross entropy， 模型初始化權重使用 YOLOv3物件偵測權重，batch size為 32，共 25 epochs，optimizer 為 Adam，學習率前 10 epochs 為   ，其餘為   。 

6 
II、 結合物件偵測與圖像顯著性

1. 設計思考 

由於我們的顯著性模型是以 YOLOv3 為基礎，這給了我們一個想法；可以利用 同一個模型結合物件辨識與顯著性預測兩個任務，因為 YOLOv3 本身就是物件辨識 模型，而在前面我們幾乎使用一樣的模型架構實踐顯著性預測，這意味者 YOLOv3 的架構應該有能力同時實踐兩個任務，我們將這個同時完成物件辨識以及顯著性預

測的模型稱作 Yoloatt (Yolo + attention -人的注意力)。 

2. 初步模型架構 

由於一開始的模型幾乎保留全部的 YOLOv3 構造，只有更改輸出端，所以 Yoloatt 的構造也只需要對輸出端做出變化，將本來 YOLOv3 的多層輸出加上兩個 channel，分別代表區域顯著圖的均值以及標準差。

3. 使用資料集

資料集方面由於缺乏完整具備兩個任務的資料集，所以我們是採用兩個任務各 一個資料集，物件辨識使用 MS COCO 2014，顯著性預測使用 SALICO[N\[3\]。](#_page15_x68.00_y173.92) 

4. 初始權重選擇

在模型的初始化權重選擇，我們有三種選擇: 

- Att 初始化:  使用之前模型(Ours)的權重，對於顯著性訓練有所幫助。
- Darknet53初始化:  使用 Darknet53分類任務的權重，對於兩者任務都沒有偏袒， 是個折衷的方案，也是我們選用的初始化方式。
- YOLOv3 初始化:  使用 YLOv3 的物件辨識任務權重，由於使用 COCO 資料集來 訓練需要大量時間才能收斂，如果用訓練好的 YOLOv3 權重，可以大幅減少訓 練時間。 
5. 損失函數合併

訓練過程中有兩種不同的 Loss: Obj-Loss, Att-Loss，我們必須給予合適的權重來 平衡兩個任務對模型的影響，否則模型會傾向執行某個任務，在此我們使用梯度衡 量的方式來決定 Loss 權重，因為模型參數的更新是透過梯度值來決定。

我們比較各層參數的平均梯度，發現 Obj-Loss 產生的梯度落在 ，而 Att-Loss 產生的梯度則落在 ，根據此結果，我們將 Loss 權重設定為 Att : Obj = 1 : 0.02。 

8 

除了使用 Darknet53 初始化來測試，我們還測試了 Att、YOLOv3 初始化下的平 均梯度，因為訓練過程中各層的參數會改變，參數改變就會影響梯度，可能會發生 當下梯度失衡但隨著訓練就復原的情況，如果在不同情況下梯度比例會自行調整， 那就不需要使用 Loss權重干預了，而使用 Att初始化代表的是訓練傾向顯著性任務， 而 YOLOv3 初始化代表的是訓練傾向物件辨識，在兩種極端的情況下，我們發現的 梯度比例依舊和 Darknet53 初始化一樣，這就代表梯度的比例失衡是不可改變的。 這是可預期的，因為兩者任務共用大部分的參數，所以決定梯度比例的關鍵在於輸 出端的不同，與前端參數更新無太大關聯。

6. 訓練方法 

對於顯著性預測訓練，我們使用原先的設置，而物件辨識則盡量保留 YOLOv3 原始設置。訓練時兩個資料集的配置如下，SALICON batch size 為 16，COCO 為 32， optimizer 為 Adam，學習率前 10 epochs 為   ，其餘為   ，一共訓練 25 epochs， epoch 的定義是以 SALICON 為依據，所以這個訓練時間對於物件辨識來說太短，不 過本次訓練並不是要直接達成目標，而是進行初步測試。

7. 結果與問題分析

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.003.jpeg)

圖  3  損失曲線，藍色為 train，紅色為 val 

根據訓練結果(圖 3)顯示，兩項任務的訓練損失都持續下降，但顯著性預測的 validation 結果卻不盡理想，顯示出顯著性任務上的 overfitting，顯著圖的 train 和 val 預測結果如圖 4。  

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.004.png)

圖  4  左半邊為 train 資料，右半邊為 val 資料，上列為平均值，下列為標準差

10 

接下來探討為何顯著性任務會 overfitting，典型 overfitting 通常源自於兩個因素， 模型參數過多以及訓練資料過少，這兩點都不適用在我們的情況，因為 Yoloatt 與 先前的模型參數量沒差多少，且使用的資料集都為 SALICON，而先前的訓練結果 (Ours)已經證明不會 overfitting。因此我們認為原因出在 Obj-Loss 對於模型權重的 影響。 

歷來的模型訓練證明了使用好的 pre-trained weight 可以避免 overfitting，所以 單做顯著性任務時使用 Darknet53 才使模型免於 overfitting，但同時搭配 Obj-Loss 訓練則是另一種情況，因為過程中權重更新會往兩種方向邁進，一種對於物件辨識 有幫助，另一種是有助於顯著性預測，但兩個方向似乎非常不同，所以當 Obj-Loss 收斂就會使 Att-Loss 發生 overfitting 的情況。 

至於為何不是 underfitting 呢，我們推測是由於 SALICON 任務相對簡單，以 Yoloatt 的參數量確實可能找到方法去適應 SALICON 的訓練資料。以另一種角度來 看，兩者任務需要的特徵性質有所差異，物件辨識需要的特徵通常比較高階，而顯 著性預測則比較低階，模型最終無法輸出兩個階級的特徵，所以導致這樣的結果。

當然我們有進行其他的實驗來驗證其他可能性，像是調整 Loss 權重比例、使 用其他初始化權重、資料強化、改變兩任務更新參數的比例、或是抽取前端低階特 徵，但上述方法都無法改善 overfitting，這使我們認知道得增加更多獨立的參數給 顯著性任務。

viii.  模型改進 

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.005.png)

圖  5   新版的 Yoloatt 架構，藍色部分為原始的 YOLOv3 構造，綠色部分為新增區塊，

用來進行顯著性預測，圖中的區塊僅代表 feature map 的長寬的變化，與實際的層 數無關。至於輸出端的 feature map 大小為(1/32、1/16、1/8)的原圖。 

我們讓新版的 Yoloatt 在顯著性預測有獨立的區塊，避免受到物件辨識影響，新 增部分僅 14 層，遠比 YOLOv3 來的淺，這是為了避免過多額外參數來增加 overfitting 可能性。考量到兩任務需要的特徵不同，新增的部分是從 feature map 降為原圖 1/8 的地方拉出，是相當淺層的位置，特徵會比較低階，更符合顯著性任務的需求。

設計成完全獨立的區塊有另一個目的，就是為了保留物件辨識的權重，然後凍 結住，達到不需要訓練物件辨識，而顯著性模型的輸入就固定使用 Yolov3 底層萃取 出來的 feature map，這樣做有兩個好處，第一是節省時間，由於訓練物件偵測需要 大量的時間，雖然之前訓練的損失曲線收斂很快，但效果還是差了一大截，如果加 上顯著性任務影響底層特徵，可能很難收斂。第二點是減少 overfitting 的機會。 

ix.  Salient Object Detection 實現 

SOD 任務是要找出影像中最受人關注的物件，可分成兩種型式(box info, segmentation)，搭配 Yoloatt 輸出的物件框資訊以及顯著圖經過處理即可得到 SOD 的 box info，而 Yoloatt 的顯著性預測具有區域平均值與區域標準差兩個部分，利用 平均值、標準差及物件框資訊，我們設計一種抓取最具顯著性的物體方法。

我們根據以下兩個特性，(1)區域平均值越高則表示該處顯著性越高，(2)標準 差越低表示該處的顯著性差異較小，因此尋找平均值最高、標準差越小的位置，即 可找出圖像中最顯著的目標點，接著判斷該點在哪些物件框內，該框即為候選框。

由於物件有前、後景導致物件框有重疊的情形，若目標點同時位於多個物件框 內，會選出數個候選框，因此必須比較各個候選框的顯著性，為此我們加入顯著性 密度評估，透過計算候選框內的顯著值總和與物件框的面積，算出候選框的顯著性 密度，從多個候選框中選出顯著性密度最高者，該框即代表最受人關注的物件。 

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.006.png)

圖  6   SOD 流程圖 

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.007.png)

圖  7 最顯著的目標點及顯著物件範例。(a)圖像輸入，(b)預測之區域平均值，(c)預測之區域標準

差，(d)物件框預測，(e)為顯著物件框。(b)、(c)、(d)圖中紅點為最顯著的目標點。(e)使用 目標點與顯著性密度所挑選出的顯著物件。

四、實  驗** 

I、 圖像顯著性任務

1. 模型評估與比較

將最高尺度 28x40 預測輸出作為顯著性的預測圖，使用雙線性內插放大至原圖 尺寸，在 SALICON 上進行顯著性指標的評估，結果如表格 1 所示，Ours 與大多 state-of-the-art 的模型相去不遠，值得一提的是我們的損失函數並未使用任何以下的 衡量指標，所以沒有過度傾向某個指標的問題。

表格  1 SALICON Validation set 顯著性指標。



|**Model** |**CC**↑** |**AUC-J**↑** |**NSS**↑** |**KLD**↓** |**SIM**↑** |
| - | - | - | - | - | - |
|**SAM-ResNet[\[4\]** ](#_page15_x68.00_y208.92)**|0\.844 |**0.886** |**3.260** |- |- |
|**SalGAN[\[5\]** ](#_page15_x68.00_y243.92)**|0\.786 |- |2\.560 |- |- |
|**EML-NE[T\[6\]** ](#_page15_x68.00_y290.92)**|0\.890 |0\.802 |2\.024 |0\.204 |0\.785 |
|**SimpleNet[\[7\]** ](#_page15_x68.00_y325.92)**|**0.907** |0\.871 |1\.926 |**0.193** |**0.797** |
|**MDNSal[\[7\]** ](#_page15_x68.00_y325.92)**|0\.899 |0\.868 |1\.893 |0\.217 |**0.797** |
|**Ours** |0\.900 |0\.868 |1\.918 |0\.249 |0\.793 |

表格  2  於 Tesla V100 32G、輸入尺寸為 224x320 下的測量結果。



|**Model** |**Params(M)** |**Mult-Adds(G)** |**Inference(ms)** |
| - | - | - | - |
|**SalGAN[\[5\]** ](#_page15_x68.00_y243.92)**|**31.8** |58\.26 |4\.5 |
|**EML-NE[T\[6\]** ](#_page15_x68.00_y290.92)**|47\.1 |**11.74** |2\.8 |
|**SimpleNet[\[7\]** ](#_page15_x68.00_y325.92)**|78\.1 |17\.77 |3\.2 |
|**Ours** |61\.5 |13\.51 |**1.9** |

除了顯著指標，我們還有計算各模型的參數量、計算量、執行時間，結果如表 格 2 所示，雖然我們的模型在參數量、計算量不是最小的，卻是執行速度最快的， 這主要是受惠於 YOLOv3 架構的高效率。 

2. 顯著性指標分析

![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.008.png)

圖  8  上排依序為輸入圖像、GT 顯著圖、注視點圖、模型預測，下排為指標損失視覺化，KLD

的亮處代表損失發生的區域，NSS、SIM、CC 亮處表示獲得分數的區域。

為了更了解這些指標對預測的意義，我們對 KLD、NSS、SIM 及 CC 進行視覺 化分析，結果如圖 8 所示，然後以下為我們的結論: 

**KLD**:  圖中亮點為損失，主要損失來自不顯著的區域預測，根據 KLD 的性值來看， 此指標對於缺少預測(FN)特別敏感，但在圖 8 中雖然是缺少預測但基本上該區域本 身就不是主要顯著區，若目的是捕捉重要顯著性來看，其實沒有太大的問題，KLD 在此時反而會成為不必要的限制。

**NSS**:  以注視點為衡量的指標，圖中亮點(非常小)為得分區，當注視點相當集中時， 只要能預測主要注視區，就可以得到高分，但當注視點相當分散時，得分往往都不 高，我們以驗證資料中 ground truth 的顯著圖和注視點圖進行 NSS 計算，發現平均 分數不過 2.22，此分數甚至低於部分 state-of-the-art 模型，顯示這個指標在注視點過 於分散時，似乎缺乏衡量效果。

**SIM**:  對主要注視點較為敏感，當主要注視點被預測到時，即可得到較高分數。 

**CC**:  與 SIM 類似，但更著重於高顯著的區域，而我們是使用區域的平均進行預測， 有助於強化顯著性較高的區域，因此在 CC 上能取得不錯的分數。

3. 實驗結論 

我們的方法僅使用低解析度的顯著圖即可有效找出高顯著的區域，並具有 YOLOv3 的高速性質，且能透過平均、標準差挑出最為集中的注視區域，可做為顯 著性的模擬或於其他應用中作為注意力機制使用。而根據我們的分析，KLD、NSS 等指標容易受到非重點區域以及注意力分散影響，所以分數不見得有實質意義。

18 

圖  9 非資料集影像的預測結果。(圖片來源:  中華民國交通部觀光局) ![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.009.png)

II、 結合物件偵測與圖像顯著性

1. 模型評估與比較

在準確度方面，Yoloatt 保留了完整的 YOLOv3 的構造與權重，所以物件辨識 的準確度與原先採用的 github[\[9\]](#_page15_x68.00_y396.92)一致，MS COCO 2014 test的 mAP (0.5 IOU)為 55.5， 至於顯著性預測的指標數值如下表所示，可以看出除了 KLD 以外都小幅變差，原 因就是出在我們的前端特徵是固定的，雖然指標下降，但在實測效果上看不出來有

多大的差異。

表格  3 Yoloatt 與 Ours 的 SALICON Validation set 顯著性指標。



|**Model** |**CC**↑** |**AUC-J**↑** |**NSS**↑** |**KLD**↓** |**SIM**↑** |
| - | - | - | - | - | - |
|**Ours** |**0.900** |**0.868** |**1.918** |0\.249 |**0.793** |
|**Yoloatt** |0\.884 |0\.866 |1\.876 |**0.229** |0\.777 |

表格 4 測試的硬體為 Tesla V100 32G，輸入尺寸為 224x320，根據測量結果， Yoloatt 比其原先的 YOLOv3 僅增加了大約 10%的參數量和計算量，與 Baseline 比 較，增加的量可說是相當微小，同時 Yoloatt 維持 YOLOv3 的高速，所以兩任務整 合能夠節省了大量的參數、計算量與執行時間。

表格  4  各類模型的參數量、計算量、執行時間測量。 



|**Model** |**Params(M)** |**Mult-Adds(G)** |**Inference (ms)** |
| - | - | - | - |
|**Baseline** |31\.79 |58\.26 |4\.5 |
|**YOLOv3** |61\.94 |13\.64 |2\.2 |
|**Yoloatt** |67\.49 |15\.25 |2\.4 |

2. 實驗結論 

在 YOLOv3 的架構基礎上，我們整合了物件偵測與圖像顯著性任務，利用分數 指標來衡量模型在雙任務預測上的表現，並計算量測模型之運算時間、參數多寡及 浮點運算量，藉此分析評估將物件偵測與圖像顯著性任務所帶來的優點與不足，根 據實驗結果，我們所提出的模型相較於 YOLOv3 僅增加約 10%的參數量和計算量， 模型相當輕量化，透過任務整合節省了大量的參數、計算量與執行時間，運算速度 與其他圖像顯著性模型相去不遠甚至較為快速，而在物件偵測與顯著性評估指標的 上也有不錯的表現，與單一任務模型的表現沒有明顯的差異。 

在衡量模型的物件與顯著性預測表現後，我們實際結合物件偵測與圖像顯著性 兩者資訊，使用顯著圖特性計算出最受人關注的物件，成功實現 Salient  Object Detection，圖 10 為實際預測範例。 

21 

圖  10 實際預測範例。 ![](Aspose.Words.37ee5294-a0cd-437d-88de-8e6a7712a3c5.010.png)


五、討  論 

我們成功實現模擬人類注意力的圖像顯著性模型，在模型準確度、運算速度、 及模型參數上與其他顯著性模型進行比較，我們的模型相對輕量且可實現即時的預 測，並且將圖像顯著性與物件偵測結合，實現顯著性物件偵測，抓取最吸引人類關 注的物件，能一定程度的近似人類的注意力機制，未來能有更加多元的顯著性結合 與應用。 

儘管人類視覺注意力可以有效幫助大腦快速進行識別、認知、追蹤等任務，但 該如何於機器視覺領域中應用與實作則需要更深入的研究與思考，在不同任務上也 有不一樣的結合方法，必須在運算效率與模型表現上進行衡量，並且於任務上結合 注意力機制後，是否會引發類似人類分心、失去注意力等現象，而造成機器的誤判 或是失誤率增加等問題，因此顯著性領域還需要更多的研究與實驗，增加其穩定性 與實用性，渴望未來能有更多理論與方法來增加與支持此領域的發展，能有更加全 面與深入的研究。 

六、結  論 

本專題第一部分著重於圖像顯著性單一任務上，我們以 YOLOv3 做為顯著性模 型的基礎，利用 YOLOv3 中的多解析度特徵預測不同解析度的顯著性圖，使用顯著 性資料集進行監督式訓練，並且分析模型的預測表現、浮點運算量、計算速度與參 數多寡，驗證結果顯示我們的模型表現與其他 state-of-the-art 的模型相去不遠，並 具有 YOLOv3 的高速性質，且能透過平均、標準差挑出最為集中的注視區域，可做 為顯著性的模擬或於其他應用中作為注意力機制使用；我們也對於四種常用的顯著 性指標進行實驗，對其指標特性與影響分數的因素進行剖析，藉此理解分數高低的 差異與指標所著重的區域，了解顯著性指標的意義。 

在第一部分的基礎上，我們將模型從單一任務擴展至物件偵測與圖像顯著性多 任務預測，同時使用物件偵測與顯著性資料集進行監督式學習，合併兩種任務的損 失函數，嘗試不同初始權重進行訓練，但起初我們遇到模型 overfitting 的問題，進 行問題分析後將模型改進，成功解決問題，我們對模型的相關性能進行實驗比較， 根據實驗結果，將多任務進行整合可以省下大量的參數與運算量，且表現上與單一 任務的模型相差不多，具有輕量且高效的特性，透過模型預測物件資訊與圖像顯著 圖，結合兩者預測結果，利用其特性進行後處理，實現 Salient Object Detection (SOD) 效果，能計算出最受關注的物件，與人類視覺系統有相當程度的近似。  

23 

七、參考資料 

1. Joseph<a name="_page15_x68.00_y106.92"></a> Redmon and Ali Farhadi. "Yolov3: An incremental improvement," CoRR, abs/1804.02767, 2018. 
1. M.<a name="_page15_x68.00_y138.92"></a> Jiang, S. Huang, J. Duan and Q. Zhao, "SALICON: Saliency in context," Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1072-1080, Jun. 2015. 
1. T.-Y.<a name="_page15_x68.00_y173.92"></a> Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. "Microsoft COCO: Common objects in context," In ECCV, 2014. 
1. M.<a name="_page15_x68.00_y208.92"></a> Cornia, L. Baraldi, G. Serra, and R. Cucchiara."Predicting human eye fixations via an lstm-based saliency attentive model", arXiv preprint arXiv:1611.09571, 2016. 
1. J.<a name="_page15_x68.00_y243.92"></a> Pan, C. Canton, K. McGuinness, N. E. O'Connor, J. Torres, E. Sayrol, et al., "SalGAN: Visual saliency prediction with generative adversarial networks," Proc. Comput. Vis. Pattern Recognit. Workshops, pp. 1-9, 2017. 
1. Sen<a name="_page15_x68.00_y290.92"></a> Jia and Neil DB Bruce. "EML-NET:An Expandable Multi-Layer NETwork for Saliency Prediction," arXiv preprint arXiv:1805.01047, 2018. 
1. N.<a name="_page15_x68.00_y325.92"></a> Reddy, S. Jain, P. Yarlagadda, and V. Gandhi. "Tidying deep saliency prediction architectures," arXiv preprint arXiv:2003.04942, 2020. 
1. A. Borji, "Saliency prediction in the deep learning era: Successes and limitations, " IEEE Trans. Pattern Anal. Mach. Intell., 2019. 
1. eriklindernoren.<a name="_page15_x68.00_y396.92"></a> (2019, May 6). PyTorch-YOLOv3.  Available:[ https://github.com/eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3)
1. niujinshuchong. (2018, Sep 3).[ SalGan_pytorch.](https://github.com/niujinshuchong/SalGan_pytorch) Available:[ https://github.com/niujinshuchong/SalGan_pytorch](https://github.com/niujinshuchong/SalGan_pytorch)
1. SenJia. (2020, May 27). EML-NET-Saliency.  Available:[ https://github.com/SenJia/EML-NET-Saliency ](https://github.com/SenJia/EML-NET-Saliency)
1. samyak0210. (2020, Jul 30). SimpleNet.  Available:[ https://github.com/samyak0210/saliency ](https://github.com/samyak0210/saliency)

八、圖片出處 

圖 9( 

120-000188 :"小琉球東港碼頭"，中華民國交通部觀光局提供，拍攝者:上允傳播 135-000325 : "富庶之地"，中華民國交通部觀光局提供，拍攝者:李金木 

120-001853 : "出發去農忙"，中華民國交通部觀光局提供，拍攝者:楊雙富 

135-000480 : "農村樂"，中華民國交通部觀光局提供，拍攝者:葉英晉 DIG-001583: "國際自由車賽"，中華民國交通部觀光局提供，拍攝者:蔡朝銘 135-001225 : "馬祖大道機場"，中華民國交通部觀光局提供，拍攝者:徐世榮 

135-000482 : "山與牛"，中華民國交通部觀光局提供，拍攝者:葉英晉 DIG-004851 : "臺灣茶"，中華民國交通部觀光局提供，拍攝者:長榮國際股份有 限公司 

` `) 
25 
